{"/advanced":{"title":"Advanced","data":{"":"직접 검색하면서 찾아봤던 자료들"}},"/advanced/chunk-oriented-tasklet--chunk-provider--chunk-processor":{"title":"Chunk Oriented Tasklet  Chunk Provider  Chunk Processor","data":{"chunkorientedtasklet-chunkprovider-chunkprocessor#ChunkOrientedTasklet, ChunkProvider, ChunkProcessor":"","참고자료#참고자료":"Spring Batch : ChunkOrientedTaskLet, ChunkProvider, ChunkProcessor\n스프링 배치 - 트랜잭션 관리를 청크 기반으로 하는 이유","목차#목차":"ChunkOrientedTasklet 이란, Chunk 단위 Transaction\nChunkOrientedTasklet 의 동작 방식\nChunkOrientedTasklet 의 주요 API\nChunkOrientedTasklet 의 Chunk 처리시 예외발생으로 인한 재시도 방식\nChunkProvider::provide()\nChunkProcessor::process()","1-chunkorientedtasklet이란-chunk-단위-transaction#1. ChunkOrientedTasklet이란, Chunk 단위 Transaction":"ChunkOrientedTaskletChunkOrientedTasklet 은 Spring Batch 에서 제공하는 Tasklet 인터페이스의 구현체입니다. 내부적으로는 Chunk 를 가지고 있기에 Chunk 기반의 프로세스 처리가 가능하며, ItemReader, ItemProcessor, ItemWriter 를 가지고 있기에 ItemReader, ItemProcessor, ItemWriter 을 활용해서 Chunk Process 처리를 합니다.내부적으로는 Repeat Template 을 가지고 있습니다.그리고 ChunkOrientedTasklet 은 Chunk 단위로 트랜잭션을 커밋합니다.\nChunk 단위 TransactionChunkOrientedTasklet 은 Chunk 단위로 트랜잭션을 커밋합니다.ChunkOrientedTasklet 은 ChunkOrientedTasklet 이 실행될 때마다 새로운 트랜잭션이 생성된 상태에서 처리가 이뤄집니다. 이 트랜잭션 단위 내에서는 Exception 이 발생한다면 Rollback 이 이뤄집니다. 참고로 예외 발생시 예외 발생 전에 이미 Commit 이 완료된 Chunk 는 그대로 유지됩니다. ChunkOrientedTasklet 은 Chunk 단위로 트랜잭션을 커밋하기 때문에 이렇게 Chunk 단위 내에서 예외가 발생하면 그 Chunk 에서 처리하던 Transaction 은 Rollback 을 하게 됩니다.Chunk 처리 중 예외가 발생해서 재시도를 해야 할 경우, 데이터를 다시 읽지 않고 버퍼(=ChunkContext)에 담아둔 데이터를 가져옵니다. 이 재시도를 할 때 내부적으로 어떻게 하는지, 데이터를 새로 읽어오는지 에 대해서는 아래의 4. 재시도 방식 : ChunkOrientedTasklet 의 Chunk 처리시 예외 발생으로 인한 재시도 방식 섹션에서 정리합니다.\n4. 재시도 방식 : ChunkOrientedTasklet 의 Chunk 처리시 예외 발생으로 인한 재시도 방식\n실패 후 재시도시 데이터를 새로 읽어올 경우 기존 데이터와 다를 수 있기에 내부적으로 ChunkContext 라고 불리는 버퍼를 사용하는데, 이 버퍼에 데이터를 어떻게 저장하는지 등과 관련된 내용을 다룹니다.","2-chunkorientedtasklet-의-동작-방식#2. ChunkOrientedTasklet 의 동작 방식":"execute()\nTaskletStep 은 execute() 메서드를 호출해서 ChunkOrientedTasklet 을 실행합니다.\nChunkOrientedTasklet::provide()\nItemReader 를 통해서 Chunk에 저장할 item을 읽어옵니다.\n이때 Chunk 내에서 처음 불러오는 것이라면 트랜잭션을 열어줍니다.\n이 데이터는 ChunkSize 만큼의 데이터입니다.\nChunkOrientedTasklet::process(inputs)\nItemReader가 읽어들인 InputChunk 를 ChunkProcessor::process() 메서드에 전달됩니다.\nItemProcessor::process()\nChunkProcessor 에서부터 Input Chunk 는 ItemProcessor::process() 에서 단건으로 처리됩니다.\nItemProcessor::process() 의 결과는 OutputChunk에 하나씩 들어갑니다.\nItemWriter::write(items)\nOutput Chunk 가 ItemWriter 에 전달되며, 이 OutputChunk 는 ItemWriter 에서는 배치(덩어리)처리 됩니다.","3-chunkorientedtasklet-의-주요-api#3. ChunkOrientedTasklet 의 주요 API":"<I,O> chunk(int chunkSize)\nchunkSize 설정\n다르게 이야기하면, commit 인터벌을 의미합니다.\n스프링 배치의 ChunkOrientedTasklet 은 chunk 단위로 트랜잭션을 새로 열기 때문에 commit 인터벌 이라는 말로 설명할 수 있습니다.\n<I,O> chunk(CompletionPolicy)\nChunk Process 를 완료하기 위한 설정 클래스를 지정합니다.\nreader()\nItemReader 구현체를 지정하는 메서드 입니다.\nwriter()\nItemWriter 구현체를 지정하는 메서드 입니다.\nprocessor()\nItemProcessor 구현체를 지정하는 메서드입니다.\nOptional 하게 지정가능하며, ItemProcessor 가 필요없다면 지정해주지 않아도 됩니다.\nstream()\n재시작 데이터를 관리하는 콜백에 대한 stream 을 지정하는 메서드 입니다.\nreaderIsTransactionalQueue()\nItem 이 JMS 와 같은 트랜잭션 트랜잭션 외부에서 읽혀지고 캐시할 것인지를 지정하는 메서드입니다.\nlistener\n리스너를 지정할 때 사용하는 메서드입니다.\nbuild\nbuild 메서드를 통해 객체를 생성합니다.","4-재시도-방식--chunkorientedtasklet-의-chunk-처리시-예외-발생으로-인한-재시도-방식#4. 재시도 방식 : ChunkOrientedTasklet 의 Chunk 처리시 예외 발생으로 인한 재시도 방식":"Chunk 처리 중 예외가 발생해서 재시도를 해야 할 경우, 데이터를 다시 읽지 않고 버퍼(=ChunkContext)에 담아둔 데이터를 가져옵니다.","내부-동작---chunkorientedtaskletexecute#내부 동작 - ChunkOrientedTasklet::execute()":"public class ChunkOrientedTasklet<I> implements Tasklet {\r\n    // ...\r\n    \r\n    public RepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) throws Exception {\r\n        // (1)\r\n        Chunk<I> inputs = (Chunk)chunkContext.getAttribute(\"INPUTS\");\r\n        \r\n        // (2)\r\n        if (inputs == null) { // (2.1) : 새로운 데이터일 경우 (캐시에 남아있는 데이터가 아닐때)\r\n          inputs = this.chunkProvider.provide(contribution); // (2.2)\r\n          if (this.buffering) { // (2.3) : buffering 이 null 이 아니라면 캐시에 작업할 chunk 를 보관\r\n            chunkContext.setAttribute(\"INPUTS\", inputs);\r\n          }\r\n        }\r\n\r\n        // (3)\r\n        this.chunkProcessor.process(contribution, inputs);\r\n        this.chunkProvider.postProcess(contribution, inputs);\r\n        \r\n        if (inputs.isBusy()) {\r\n          logger.debug(\"Inputs still busy\");\r\n          return RepeatStatus.CONTINUABLE;\r\n        } else {\r\n          // (4) 아무 문제 없이 현재 처리를 완료했다면, Chunk 처리 작업이 캐싱된 것을 제거\r\n          chunkContext.removeAttribute(\"INPUTS\");\r\n          chunkContext.setComplete();\r\n          if (logger.isDebugEnabled()) {\r\n            logger.debug(\"Inputs not busy, ended: \" + inputs.isEnd());\r\n          }\r\n\r\n          // (5) 읽을 Item 이 더 존재하는지 체크해서 그 상태를 RepeatStatus 로 return\r\n          return RepeatStatus.continueIf(!inputs.isEnd());\r\n        }\r\n\t}\r\n}\n(1) : Chunk<I> inputs = (Chunk)chunkContext.getAttribute(\"INPUTS\");\nChunkContext 라는 것은 캐시같은 개념입니다.\nChunkContext 에 이전에 실행했던 Chunk 가 남아있는지를 getAttribute(\"INPUTS\") 를 통해 검사합니다.\ngetAttribute(\"INPUTS\") 로 조회한 Chunk<I> 가 존재한다면, 예전에 실행하던 Chunk 가 남아있다는 의미입니다.\nChunk 가 남아있다는 것은 작업에 실패했다거나 그런 이슈로 인해 ChunkContext 에 저장되어있다는 것을 의미합니다.\n(2)\n(2.1) : if (inputs == null)\n새로운 데이터일 경우 (캐시에 남아있는 데이터가 아닐때) 를 의미합니다.\n(2.2) : inputs = this.chunkProvider.provide(contribution);\nChunkProvider 의 provide(contribution) 메서드를 호출해서 현재 청크를 Process, Write 를 하는 단계로 넘어갑니다.\n(2.3) if (this.buffering) {...}\nbuffering 이 null 이 아니라면 캐시에 작업할 Chunk 를 보관합니다.\n이렇게 해서 저장한 작업인 Chunk 는 정상적으로 처리가 된다면 캐시(=ChunkContext)에서 삭제를 하는데, (4) 의 단계가 작업이 정상적으로 종료된 후 캐시(=ChunkContext)에서 삭제를 하는 단계입니다.\n(3) : this.chunkProcessor.process(contribution, inputs);\nInputChunk 를 OutputChunk 로 가공해줍니다.\n(4) : chunkContext.removeAttribute(\"INPUTS\");\n아무 문제 없이 현재 처리를 완료했다면, Chunk 처리 작업이 캐싱된 것을 제거합니다.\n(5) : return RepeatStatus.continueIf(!inputs.isEnd());\n읽을 Item 이 더 존재하는지 체크해서 그 상태를 RepeatStatus 로 return 합니다.","chunkproviderprovide#ChunkProvider::provide()":"ChunkProvider interface 는 provide(), postProcess() 메서드를 제공합니다.\npackage org.springframework.batch.core.step.item;\r\n\r\nimport org.springframework.batch.core.StepContribution;\r\n\r\npublic interface ChunkProvider<T> {\r\n  Chunk<T> provide(StepContribution var1) throws Exception;\r\n\r\n  void postProcess(StepContribution var1, Chunk<T> var2);\r\n}\n이 ChunkProvider interface 를 구현하는 구현체들은 다음과 같습니다.\nFaultTolerantChunkProvider\nJsrChunkProvider\nSimpleChunkProvider\n이번 문서에서는 위의 구현체 들 중 SimpleChunkProvider 클래스의 provide() 메서드의 동작을 살펴봅니다.","simplechunkproviderprovide#SimpleChunkProvider::provide()":"public class SimpleChunkProvider<I> implements ChunkProvider<I> {\r\n    public Chunk<I> provide(final StepContribution contribution) throws Exception {\r\n        // (1) \r\n        final Chunk<I> inputs = new Chunk();\r\n        \r\n        // (2) repeatOperations.iterate(new RepeatCallback(){...})\r\n        this.repeatOperations.iterate(new RepeatCallback() {\r\n          public RepeatStatus doInIteration(RepeatContext context) throws Exception {\r\n            I item = null;\r\n            Timer.Sample sample = Timer.start(Metrics.globalRegistry);\r\n            String status = \"SUCCESS\";\r\n\r\n            label45: {\r\n              RepeatStatus var6;\r\n              try {\r\n                // (3) \r\n                item = SimpleChunkProvider.this.read(contribution, inputs);\r\n                break label45;\r\n              } catch (SkipOverflowException var10) {\r\n                status = \"FAILURE\";\r\n                var6 = RepeatStatus.FINISHED;\r\n              } finally {\r\n                SimpleChunkProvider.this.stopTimer(sample, contribution.getStepExecution(), status);\r\n              }\r\n\r\n              return var6;\r\n            }\r\n\r\n            if (item == null) {\r\n              // (4)\r\n              inputs.setEnd();\r\n              return RepeatStatus.FINISHED;\r\n            } else {\r\n              // (5) \r\n              inputs.add(item);\r\n              contribution.incrementReadCount();\r\n              return RepeatStatus.CONTINUABLE;\r\n            }\r\n          }\r\n        });\r\n        \r\n        return inputs;\r\n\t}\r\n}\n(1) : Chunk 객체를 새로 생성합니다. ChunkProvider 는 provide() 메서드가 호출될 때마다 새로운 Chunk 객체를 생성합니다.\n(2)\nIterator 패턴 기반인 RepeatOperations 를 기반으로 iterate 를 수행합니다.\n내부적으로는 doInIteration 클래스를 구현하고 있습니다.\n(3)\nSimpleChunkProvider 내의 read(Contributuion, inputs) 를 통해 소스로부터 item 을 읽어옵니다.\n(4)\n읽어온 item 이 없을 경우에는 종료하는데 inputs.setEnd() 를 호출해서 종료합니다.\n(5)\n읽어온 item 이 있는 경우 Input Chunk 인 inputs 에 넣어줍니다. (inputs.add(item) )","chunkprocessor#ChunkProcessor":"ChunkProcessor interface 는 process() 메서드를 제공합니다.\npackage org.springframework.batch.core.step.item;\r\n\r\nimport org.springframework.batch.core.StepContribution;\r\n\r\npublic interface ChunkProcessor<I> {\r\n  void process(StepContribution var1, Chunk<I> var2) throws Exception;\r\n}\n이 ChunkProcessor interface 를 구현하는 구현체들은 아래와 같은 구현체들이 있습니다.\nFaultTolerantChunkProcessor\nJsrChunkProcessor\nJsrFaultTolerantChunkProcessor\nSimpleChunkProcessor\n이번 문서에서는 위의 구현체 들 중 SimpleChunkProcessor 클래스의 process() 메서드의 동작을 살펴봅니다.","simplechunkprocessorprocess#SimpleChunkProcessor::process()":"public class SimpleChunkProcessor<I, O> implements ChunkProcessor<I>, InitializingBean {\r\n    // ...\r\n    public final void process(StepContribution contribution, Chunk<I> inputs) throws Exception {\r\n        this.initializeUserData(inputs);\r\n        \r\n        if (!this.isComplete(inputs)) {\r\n            // (1)\r\n            Chunk<O> outputs = this.transform(contribution, inputs); \r\n            // (2) \r\n            contribution.incrementFilterCount(this.getFilterCount(inputs, outputs)); \r\n            // (3)\r\n            this.write(contribution, inputs, this.getAdjustedOutputs(inputs, outputs));\r\n        }\r\n    }\r\n}\n(1) : transform(contribution, inputs);\ntransform() 메서드는 nputChunk 를 itemProcessor 에 전달해서 Output Chunk 를 만들어오는 역할을 수행합니다.\ntransform() 메서드는 아래에서 설명합니다.\n(2) : incrementFilterCount()\nProcessor 의 Process 과정에서 filter 에 부합하는 유효한 값들만 필터링하며 카운트를 합니다.\n(3) : write(contribution, inputs, this.getAdjustedOutputs(inputs, outputs));\n(1), (2) 에서 구한 OutputChunk 를 ItemWriter 에 전달해줍니다.\nwrite() 메서드는 아래에서 설명합니다.","simplechunkprocessortransform#SimpleChunkProcessor::transform()":"참고 : SimpleChunkProcessor\n위에서 살펴본 ChunkProcessor::process() 메서드는 transform() 메서드를 호출하고 있는데, transform() 메서드의 내부 정의는 아래와 같이 정의되어 있습니다.\npublic class SimpleChunkProcessor<I, O> implements ChunkProcessor<I>, InitializingBean {\r\n    // ...\r\n\r\n    protected Chunk<O> transform(StepContribution contribution, Chunk<I> inputs) throws Exception {\r\n        Chunk<O> outputs = new Chunk(); // (2)\r\n        Chunk<I>.ChunkIterator iterator = inputs.iterator();\r\n\r\n        while(iterator.hasNext()) {\r\n          I item = iterator.next();\r\n          Timer.Sample sample = BatchMetrics.createTimerSample();\r\n          String status = \"SUCCESS\";\r\n\r\n          Object output;\r\n          try {\r\n            output = this.doProcess(item); // (1)\r\n          } catch (Exception var13) {\r\n            inputs.clear();\r\n            status = \"FAILURE\";\r\n            throw var13;\r\n          } finally {\r\n            this.stopTimer(sample, contribution.getStepExecution(), \"item.process\", status, \"Item processing\");\r\n          }\r\n\r\n          if (output != null) {\r\n            outputs.add(output); // (2)\r\n          } else {\r\n            iterator.remove(); // (2)\r\n          }\r\n        }\r\n\r\n        return outputs; // (2)\r\n  \t}\r\n}\n(1)\ntransform 메서드는 while 문을 통해서 doProcess(item) 을 itemSize 만큼 처리해줍니다.\n즉, item 을 하나씩 단건으로 Processing 합니다.\n(2)\n새로운 Chunk<O> outputs 객체를 만든 후, 이 outputs에 처리 결과를 add() 하거나, remove() 해서 처리 대상 데이터에서 필터링합니다.\n그리고 이 Chunk<O> outputs 을 return 합니다.","simplechunkprocessorwrite#SimpleChunkProcessor::write()":"public class SimpleChunkProcessor<I, O> implements ChunkProcessor<I>, InitializingBean {\r\n    // ...\r\n    \r\n    protected void write(StepContribution contribution, Chunk<I> inputs, Chunk<O> outputs) throws Exception {\r\n        Timer.Sample sample = BatchMetrics.createTimerSample();\r\n        String status = \"SUCCESS\";\r\n\r\n        try {\r\n          // (1)\r\n          this.doWrite(outputs.getItems());\r\n        } catch (Exception var10) {\r\n          inputs.clear();\r\n          status = \"FAILURE\";\r\n          throw var10;\r\n        } finally {\r\n          this.stopTimer(sample, contribution.getStepExecution(), \"chunk.write\", status, \"Chunk writing\");\r\n        }\r\n\r\n        contribution.incrementWriteCount(outputs.size());\r\n\t}\r\n    \r\n    // ...\r\n    protected final void doWrite(List<O> items) throws Exception {\r\n        if (this.itemWriter != null) {\r\n            try {\r\n                this.listener.beforeWrite(items);\r\n                // (2)\r\n                this.writeItems(items);\r\n                this.doAfterWrite(items);\r\n            } catch (Exception var3) {\r\n                this.doOnWriteError(var3, items);\r\n                throw var3;\r\n\t\t\t}\r\n        }\r\n\t}\r\n}\n(1)\ndoWrite() 메서드에 outputs.getItems() 를 넘겨줍니다. outputs.getItems() 는 Output Chunk 의 List 입니다.\n(2)\ndoWrite(...) 메서드에서는 writeItems(items) 메서드에 items 전체를 넘겨줍니다.\n이로 미루어볼수 있는 것은 ItemWriter 는 데이터를 배치단위(덩어리)로 한번에 처리한 다는 사실을 알수 있습니다.","spring-batch-가-트랜잭션-관리를-청크-기반으로-하는-이유#Spring Batch 가 트랜잭션 관리를 청크 기반으로 하는 이유":"참고 :\n스프링 배치 - 트랜잭션 관리를 청크 기반으로 하는 이유\n스프링 배치를 사용하면서 겪은 것들\n제 생각은 이렇습니다. 스프링의 Transactional AOP를 스프링 배치에 적용할 수 없었기 때문이었을 것입니다. Chunk 단위로 커밋을 해야 하는데, 이 트랜잭션 프록시라는게 클래스나 메서드 레벨에 적용이 가능하지 Chunk 에 적용하기는 구조상으로 쉽지 않았기에 스프링 배치 내에서 자체적으로 AOP 트랜잭션을 Chunk 기반으로 동작하도록 내부를 구현해둔게 아닐까 싶습니다.배치는 대용량 데이터를 다루는데, 대용량 데이터를 다룰때 컴퓨팅 자원이 부족해질 수 있습니다. 너무 큰 데이터를 한번에 로드해서 메모리가 부족해지는 현상을 겪을 수 있습니다. 따라서 데이터를 Chunk 단위로 읽어서 트랜잭션 동작을 수행하게 끔 하는 방식으로 내부 메커니즘이 구현되어 있습니다.이렇게 하면, 커밋이 Fetch Size 만큼 이뤄집니다. Chunk 단위로 커밋이 이뤄지며, 이 Chunk 의 크기는 Fetch Size 를 의미합니다.흔히 인터넷에서는 commit Interval 단위로 트랜잭션이 처리된다는 말을 자주 씁니다. 제 경우에는 commit interval 이라는 용어를 쓰는게 너무 용어 같아서 좋아하는 표현은 아닙니다.\n이렇게 Chunk 단위로 커밋을 수행하면  ItemReader, ItemProcessor, ItemWriter 를 사용가능해지는데, Tasklet 기반의 처리방식과 비교했을 때 장점은 아래와 같습니다.\nskip, retry, no-rollback 정책기반의 동작 수행이 가능해지고\nItemReader 를 이용해서 조금 더 유연한 처리를 할 수 있습니다. (tasklet 기반 처리에서는 지원되지 않는 부분)\nmulti thread 를 위한 기능 역시 사용할 수 있습니다.\n즉, 청크 기반 방식의 ItemReader,Processor,Writer 를 사용하는 방식에서 재시도,롤백, skip 을 세부적으로 지정할 수 있기에 청크 기반 처리를 수행하며, 복잡한 배치 작업에서 일반적인 스프링에서의 트랜잭션 후처리 방식인  TransactionCallback 방식을 사용할 경우 데드락에 걸리는 이슈가 있기 때문에 청크 단위로 트랜잭션을 잡은 것입니다."}},"/advanced/etc":{"title":"Etc","data":{"etc---기타-지식들#etc - 기타 지식들":"","jobrepository#JobRepository":"참고자료 : Spring Batch : JobRepository\nJobRepository 는 Database 에 Job, Step 에 대한 Instance 또는 Execution Instance, Context 그리고 Parameter 들을 Database 에 저장하거나, 수정, 조회, 삭제를 하는 역할을 담당합니다.우리가 JobRepository 를 선언하지도 않았고, @EnableJpaRepositories 에 직접 등록하지도 않았는데, JobRepository 를 사용할 수 있는 이유는 @EnableBatchProcessing 애노테이션 덕분입니다. @EnableBatchProcessing 애노테이션은 JobRepository 를 스프링 빈으로 등록합니다.만약 사용자 정의 JobRepository 를 사용하려 한다면, BatchConfigurer/BasicBatchConfigurer 를 implements 또는 extends 해서 사용하면 됩니다. JobRepository 는 내장 메모리(embedded), DB 용도로 설정 역시 따로 지정할 수 있습니다.\n트랜잭션\nJobRepository 는 트랜잭션을 AOP 기술로 처리해주는데, 스프링의 @Transactional 이 아니라, Batch 내에서의 트랜잭션을 처리하는 별도의 AOP가 있습니다.\n트랜잭션 격리 수준은 Serializable (=가장 엄격한 수준)이며, 더 낮은 격리 수준으로 지정하는 것 역시 가능합니다.\n메타테이블의 Prefix (접두사) 를 벼경할 수 있습니다. 기본 값은 BATCH 입니다."}},"/basic":{"title":"Basic","data":{"":"필수적으로 알아야 하는 내용들"}},"/basic/1-setting-hellobatch":{"title":"1 Setting Hellobatch","data":{"setting--hellospring-batch#Setting + Hello,Spring Batch":"","setting#Setting":"프로젝트 셋업\nstart.spring.io 에서 lombok, mysql, spring-batch 등을 선택합니다.\nDatabase\nintellij 내에서 localhost:3306 연결하고, 접속 테스트시 에러나면, Timezone 세팅하는 버튼을 클릭해서 timezone 을 세팅합니다.\ndatabase 생성\ncreate database spring_batch;","hello-spring-batch#Hello, Spring Batch":"간단한 HelloJob 을 생성하고 구동함\nJob\n배치의 실행 단위\nJobBuilderFactory\n스프링 배치 설정에서 Bean 으로 등록되어 있는 객체\nRunIdIncrementer\nJob 을 실행할 때마다 파라미터 ID 를 자동으로 생성해주는 클래스\njobBuilderFactory.get(\"helloJob\")\nJob name 을 helloJob 으로 지었는데, job name 은 Spring Batch 를 실행할 수 있는 Key 역할을 한다.\nstart(Step)\njob 실행시 최초로 실행될 클래스를 지정하는 메서드\nStep\nJob 의 실행 단위\n하나의 Job 은 1개 이상의 Step 을 가질 수 있습니다.\nStep 역시 Job 처럼 Bean 으로 만들어야 합니다.\nJob 과 Step 에 대해서는 뒤에서 자세히 다룹니다.\nchunk 기반 tasklet 실행\n예제 참고. tasklet 기반 예제를 굳이 여기에 적으면 낭비가 심해서 생략합니다.\nTODO : 예제 코드 링크 추가할 것\n애플리케이션 실행시 HelloJob 배치가 실행됩니다. 만약 다른 Job 이 있는 상태에서 이 HelloJob 배치를 실행하면 모든 Job 이 실행된다.모든 Job 을 실행하는 것이 아니라 특정 Job 만 실행하려 할 경우 아래 설정을 해준다.Run/Debug Configuration\nEdit Configurations ...\nBuild and run > Modify options 클릭\nJava > Program arguments 선택\nConfigurations 창에서 아래의 옵션을 입력\n--spring.batch.job.names={실행할 Job name}\nApply > Ok\n위와 같이 설정해주면 Program Argument 로 지정한 Batch 만 실행하겠다는 설정이되어서 실행 시 Program Argument 로 지정한 Batch 만 실행되게 된다.그리고 직접 인자값으로 지정하지 않은 Job 까지 실행되는 것을 막으려면 application.yml 파일 내에 아래와 같이 설정해준다.\nspring.batch.job.names: ${job.name.NONE}\nspring:\r\n  batch:\r\n    job:\r\n      names: ${job.name:NONE}\n또는 아래와 같이 설정해준다.\n(Spring Boot Configuration Processor 가 추천해준 속성이다.)\nspring.batch.job.enabled: ${spring.batch.job.names:NONE}\nspring:\r\n  batch:\r\n    job:\r\n      enabled: ${spring.batch.job.names:NONE}","job-step#Job, Step":"Job\nBatch 의 실행 단위\nStep\nJob 의 실행 단위"}},"/basic/csv-read-write-example":{"title":"Csv Read Write Example","data":{"csv-파일-읽기쓰기-예제#CSV 파일 읽기/쓰기 예제":"","csv-파일-생성#csv 파일 생성":"아래와 같은 csv 파일을 src/main/resource 아래에 작성합니다.src/main/resources/member-test-1.csv\nid,name,email\r\n1,aaaaa,aaaaa@email.com\r\n2,bbbbb,bbbbb@email.com\r\n3,ccccc,ccccc@email.com\r\n4,ddddd,ddddd@email.com\r\n5,eeeee,eeeee@email.com\r\n6,fffff,fffff@email.com\r\n7,ggggg,ggggg@email.com\r\n8,hhhhh,hhhhh@email.com\r\n9,iiiii,iiiii@email.com\r\n10,jjjjj,jjjjj@email.com","csv-파일-읽기#csv 파일 읽기":"package com.example.batch_sample.member.batch;\r\n\r\nimport com.example.batch_sample.global.reader.LinkedListItemReader;\r\nimport com.example.batch_sample.member.entity.MemberEntity;\r\nimport com.example.batch_sample.member.entity.factory.MemberEntityFactory;\r\nimport com.example.batch_sample.member.fixtures.MemberEntityFixtures;\r\nimport java.util.stream.Collectors;\r\nimport lombok.RequiredArgsConstructor;\r\nimport lombok.extern.slf4j.Slf4j;\r\nimport org.springframework.batch.core.Job;\r\nimport org.springframework.batch.core.Step;\r\nimport org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\r\nimport org.springframework.batch.core.configuration.annotation.JobScope;\r\nimport org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\r\nimport org.springframework.batch.core.launch.support.RunIdIncrementer;\r\nimport org.springframework.batch.item.ItemWriter;\r\nimport org.springframework.batch.item.file.FlatFileItemReader;\r\nimport org.springframework.batch.item.file.builder.FlatFileItemReaderBuilder;\r\nimport org.springframework.batch.item.file.mapping.DefaultLineMapper;\r\nimport org.springframework.batch.item.file.transform.DelimitedLineTokenizer;\r\nimport org.springframework.context.annotation.Bean;\r\nimport org.springframework.context.annotation.Configuration;\r\nimport org.springframework.core.io.ClassPathResource;\r\n\r\n@Slf4j\r\n@Configuration\r\n@RequiredArgsConstructor\r\npublic class CsvFileSampleJobConfig {\r\n\r\n  private final JobBuilderFactory jobBuilderFactory;\r\n  private final StepBuilderFactory stepBuilderFactory;\r\n\r\n  private static final int CHUNK_SIZE = 5;\r\n\r\n  @Bean\r\n  public Job CsvFileSampleJob() throws Exception {\r\n    return this.jobBuilderFactory.get(\"CSV_FILE_IO_SAMPLE_JOB\")\r\n        .incrementer(new RunIdIncrementer())\r\n        .start(this.CsvFileSampleStep())\r\n        .build();\r\n  }\r\n\r\n  @Bean\r\n  @JobScope\r\n  public Step CsvFileSampleStep() throws Exception {\r\n    return this.stepBuilderFactory.get(\"CSV_FILE_IO_SAMPLE_STEP\")\r\n        .<MemberEntity, MemberEntity>chunk(CHUNK_SIZE)\r\n        .reader(csvFileItemReader())\r\n        .writer(loggingWriter())\r\n        .build();\r\n  }\r\n\r\n  public FlatFileItemReader<MemberEntity> csvFileItemReader() throws Exception {\r\n    // tokenizer\r\n    DelimitedLineTokenizer tokenizer = new DelimitedLineTokenizer();\r\n    tokenizer.setNames(\"id\", \"name\", \"email\");\r\n\r\n    // lineMapper\r\n    DefaultLineMapper<MemberEntity> lineMapper = new DefaultLineMapper<>();\r\n    lineMapper.setLineTokenizer(tokenizer);\r\n\r\n    // fieldMapping 지정\r\n    lineMapper.setFieldSetMapper(fieldSet -> {\r\n      int id = fieldSet.readInt(\"id\");\r\n      String name = fieldSet.readString(\"name\");\r\n      String age = fieldSet.readString(\"email\");\r\n\r\n      return new MemberEntityFactory().readFrom(Long.parseLong(String.valueOf(id)), name, age);\r\n    });\r\n\r\n    // FlatMapItemReader 정의\r\n    FlatFileItemReader<MemberEntity> itemReader = new FlatFileItemReaderBuilder<MemberEntity>()\r\n        .name(\"CSV_FILE_ITEM_READER\") // READER NAME 지정\r\n        .encoding(\"UTF-8\") // encoding 방식 지정\r\n        .resource(new ClassPathResource(\"member-test-1.csv\")) // Classpath 내의 test.csv 파일\r\n        .linesToSkip(1) // 첫 1줄은 skip (제목)\r\n        .lineMapper(lineMapper) // lineMapper 지정\r\n        .build();\r\n\r\n    itemReader.afterPropertiesSet();\r\n\r\n    return itemReader;\r\n  }\r\n\r\n  private ItemWriter<MemberEntity> loggingWriter(){\r\n    return items -> {\r\n      String s = items.stream().map(MemberEntity::getName).collect(Collectors.joining(\", \"));\r\n      log.info(s);\r\n    };\r\n  }\r\n\r\n}\n출력결과","csv-파일-쓰기-예제#csv 파일 쓰기 예제":"정리 중... 으어어"}},"/basic/flat-file-item-reader":{"title":"Flat File Item Reader","data":{"flatfileitemreader#FlatFileItemReader":"참고자료 : https://tonylim.tistory.com/434?category=951024이것 외에도 FlatFileItemReader 는 다양한 예제와 옵션들이 많다. 필요한 내용들을 추려서 7월 내로 꼭 요약해둬야지!!\ne.g.\n@Bean\r\npublic FlatFileItemReader itemReader(){\r\n    return new FlatFileItemReaderBuilder<T>()\r\n        .name(String name) // ExecutionContext 내에서 구분하기 위한 이름 (ExecutionContext 내에서 key 로 구분됨)\r\n        .resource(Resource) // 읽어야 할 리소스 설정\r\n        .delimited().delimiter(\"|\") // 파일의 구분자(delimiter)를 기준으로 파일을 읽어들인다.\r\n        .fixedLength() // 파일의 고정 길이를 기준으로 파일을 읽어들이도록 지정\r\n        .addColumns(Range...) // 고정 길이 범위를 지정\r\n        .names(String [] fieldNames) // LineTokenizer 로 구분된 라인의 항목을 객체의 필드명과 매핑\r\n        .targetType(Class class) // 라인의 각 항목과 매핑할 객체 타입 지정\r\n        .addComment(String comment) // 무시할 라인의 Comment 기호 지정\r\n        .strict(boolean) // 라인을 읽거나 토크나이징 할때 Parsing 예외가 발생하지 않도록 검증 생략하도록 설정\r\n        .encoding(String encoding) // File 인코딩\r\n        .linesToSkip(int linesToSkip) // 읽어들일 때 첫 줄로부터 무시할 라인수 지정\r\n        .saveState(boolean) // 상태 정보를 저장할 것인지 설정\r\n        .setLineMapper(LineMapper) // LineMapper 객체 설정 \r\n        .setFieldSetMapper(FieldSetMapper) // FieldSetMapper 객체 설정\r\n        .setLineTokenizer(LineTokenizer) // LineTokenizer 객체 설정\r\n        .build();\r\n}"}},"/":{"title":"Introduction","data":{}},"/info":{"title":"Info","data":{"":"개인적으로 접했던 좋은 자료들\nhttps://ojt90902.tistory.com/category/Spring/Spring%20Batch\n스프링 배치를 사용하면서 겪은 것들\ndocs.spring.io\nBatch Processing and Transactions\nSpring Batch Documentation\njob.html\nschema-appendix.html\n스프링 배치 트랜잭션 단위 에 대해 검색해봤던 내용들\n청크 배치 관련 write commit 및 rollback 관련 질문 있습니다.\nSpring Batch 트랜잭션 질문입니다.\nSpring Batch 는 어떻게 Chunk 지향 처리를 하고, Transaction 을 언제 걸까\nSpring Batch TransactionManager 개념, 구현체 종류와 특징"}},"/basic/job-config-live-template-creation":{"title":"Job Config Live Template Creation","data":{"jobconfig-live-template-만들기#JobConfig Live Template 만들기":"원하는 패키지에 원하는 이름으로 JobConfig를 생성합니다.\r\n그리고 아래와 같이 코드를 작성합니다.\n필요한 코드들을 비워두었기에 지금 상태는 컴파일 에러가 나는 것이 정상입니다.\nimport lombok.RequiredArgsConstructor;\r\nimport org.springframework.batch.core.Job;\r\nimport org.springframework.batch.core.Step;\r\nimport org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\r\nimport org.springframework.batch.core.configuration.annotation.JobScope;\r\nimport org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\r\nimport org.springframework.batch.core.launch.support.RunIdIncrementer;\r\nimport org.springframework.context.annotation.Bean;\r\nimport org.springframework.context.annotation.Configuration;\r\n\r\n@Configuration\r\n@RequiredArgsConstructor\r\npublic class TemplateJobConfig {\r\n  private final JobBuilderFactory jobBuilderFactory;\r\n  private final StepBuilderFactory stepBuilderFactory;\r\n\r\n  @Bean\r\n  public Job job(){\r\n    return this.jobBuilderFactory.get(\"\")\r\n        .incrementer(new RunIdIncrementer())\r\n        .start(this.step())\r\n        .build();\r\n  }\r\n\r\n  @Bean\r\n  @JobScope\r\n  public Step step(){\r\n    return this.stepBuilderFactory.get(\"\")\r\n        .chunk()\r\n        .reader()\r\n        .processor()\r\n        .writer()\r\n        .build();\r\n  }\r\n}\nFile → Save File as Template ... 을 선택합니다.\n만약 위 메뉴가 안보인다면 Tools → Save File as Template ... 을 선택하시면 됩니다.\n이후 Live Template 을 편집하는 팝업윈도우가 나타나는데 내용을 아래와 같이 정의해줍니다. (위의 코드에서 Bean 이름, Job 이름 등 변수로 지정해줄 내용을 따로 변수로 지정해줬습니다.)\npackage ${PACKAGE_NAME};\r\n\r\nimport lombok.RequiredArgsConstructor;\r\nimport org.springframework.batch.core.Job;\r\nimport org.springframework.batch.core.Step;\r\nimport org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\r\nimport org.springframework.batch.core.configuration.annotation.JobScope;\r\nimport org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\r\nimport org.springframework.batch.core.launch.support.RunIdIncrementer;\r\nimport org.springframework.context.annotation.Bean;\r\nimport org.springframework.context.annotation.Configuration;\r\n\r\n@Configuration\r\n@RequiredArgsConstructor\r\npublic class ${NAME} {\r\n  private final JobBuilderFactory jobBuilderFactory;\r\n  private final StepBuilderFactory stepBuilderFactory;\r\n\r\n  @Bean\r\n  public Job ${JOB_BEAN_NAME}(){\r\n    return this.jobBuilderFactory.get(\"${JOB_NAME}\")\r\n        .incrementer(new RunIdIncrementer())\r\n        .start(this.${STEP_BEAN_NAME}())\r\n        .build();\r\n  }\r\n\r\n  @Bean\r\n  @JobScope\r\n  public Step ${STEP_BEAN_NAME}(){\r\n    return this.stepBuilderFactory.get(\"${STEP_NAME}\")\r\n        .chunk()\r\n        .reader()\r\n        .processor()\r\n        .writer()\r\n        .build();\r\n  }\r\n}\n작성을 다 하고 나면 아래와 같이 Template 의 Name 을 지정해준 후 OK 버튼을 눌러서 저장합니다.\r\n만들었던 TemplateJobConfig Class 는 삭제해줍니다.\r\n새로운 Job 을 만듭니다.\r\nFile → New → SpringBatchJob 을 입력합니다.\r\n또는\r\nCtrl + N → SpringBatchJob 을 입력합니다.\nSpringBatchJob 은 방금 생성한 Spring Batch Job Live Template 입니다.\n아래와 같이 원하는 이름을 입력해주고 OK 버튼을 누릅니다.\r\nJob 의 Bean Name 과 Job Name을 다르게 할 필요는 없으며 보통은 같은 이름으로 설정해주는 것이 보통이지만, 이번 문서에서는 모두 다르게 지정했습니다.작성을 완료하고 나면 아래와 같이 기본적인 내용이 채워진 Job Config 파일을 확인할 수 있습니다."}},"/basic/2.1-architecture":{"title":"2.1 Architecture","data":{"기본-구조#기본 구조":"참고 : https://docs.spring.io/spring-batch/docs/4.3.5/reference/html/job.html","joblauncher-jobrepository-job-chunk-tasklet--itemreader-itemprocessor-itemwriter#JobLauncher, JobRepository, Job, Chunk, Tasklet,  ItemReader, ItemProcessor, ItemWriter":"","joblauncher#JobLauncher":"Bean 을 생성만 했을 뿐인데 Batch가 실행될 수 있는데, 원하지 않는 잡들이 실행되는 것을 막으려면 application.yml 에 아래의 설정을 추가하면 됩니다.\nspring.batch.job.enabled: ${spring.batch.job.names:NONE}\nspring.batch.job.names: ${job.name.NONE}\nSpring Batch 는 Job 타임에 Bean 이 생성되면 JobLauncher 객체에 의해서 Job 을 수행합니다.\nJobLauncher 는 Job 을 실행하고, Job 은 Step 을 수행합니다.","jobrepository#JobRepository":"DB or Memory 에 스프링 배치가 실행될 수 있도록 배치의 메타데이터를 관리하는 역할을 수행","job#Job":"배치의 실행단위 (중요!!)\nJob 은 JobLauncher 에 의해 실행됩니다.\nJob 은 여러 개의 Step 을 실행하며, Flow 를 관리할 수 있습니다.\ne.g. Step A → on 조건 B → Step B\n이렇게 여러개의 Step 을 Flow 로 실행하는 것을 Job Flow 라고 부릅니다.","chunk-tasklet#Chunk, Tasklet":"예를 들어 100만건의 데이터를 처리해야 하는 작업이 있다고 하자.만약 100만건의 데이터에 대해 원하는 작업을 할 때 컴퓨터 자원에 문제가 없다면?\nTasklet 처리를 해도 무방합니다.\n만약 100만건의 데이터에 대해 원하는 작업을 할 때 컴퓨터 자원에 문제가 있다면?\nChunk 기반의 처리를 하는 것이 권장됩니다.\n1만건 Size 의 Chunk 를 만들어서 이 Chunk 를 페이징 기반의 처리를 하는 방식으로 전환한다면, 메모리가 부족해져서 프로그램이 멈추는 현상 등을 방지할 수 있습니다.\n뒤에서 정리하겠지만, 가급적이면 페이징 사이즈는 Chunk Size 와 동일하게 하는 것이 권장됩니다.\nTasklet 도 나눠서 처리하는 것을 수동으로 작성할 수 있지만 대용량 데이터를 처리시에는 Chunk 기반 처리방식이 더 활용성이 높고 ItemReader, ItemProcessor, ItemWriter 등을 통해서 딱딱 떨어지게끔 처리하면서 중간에 어디까지 실행했는지에 대한 Context 를 저장하거나 이력을 보관할 수 있기에 가급적이면 Chunk 기반의 처리를 하도록 작성하는 것을 추천되는 편입니다.","itemreader-itemprocessor-itemwriter#ItemReader, ItemProcessor, ItemWriter":"ItemReader\n배치 처리를 해야 하는 대상 객체를 읽어들이는 역할\ne.g. FlatFileItemReader, JdbcPagingItemReader, JpaPagingItemReader\nItemProcessor\nItemReader 로부터 읽어들인 데이터를 ItemWriter 로 보내기 전에 Processing 또는 Filtering 작업을 수행하는 역할\nnull 을 리턴하면 그 데이터는 필터링 되어 ItemWriter 로는 전달되지 않습니다.\nItemProcessor 는 Optional 이며, 생략가능하다. 즉, Step 구성시 ItemReader, ItemWriter 로만 구성하는 것도 가능합니다.\nItemWriter\nItemProcessor 로부터 전달된 객체를 이용해서 데이터를 저장하거나, 메시지큐에 데이터를 전송하는 등과 같은 Write 하는 역할을 담당합니다.","sprng-batch-tables#Sprng Batch Tables":"","table-명세#Table 명세":"Spring Batch 는 배치 실행 기록과, 배치 실행 결과를 저장하는 테이블들을 가지고 있습니다. 이 테이블들을 Meta 테이블이라고 흔히 이야기합니다.참고 : https://docs.spring.io/spring-batch/reference/_images/meta-data-erd.png\nBATCH_JOB_INSTANCE\n위의 ERD 에서 BATCH_JOB_INSTANCE 테이블을 자세히 보면 JOB_NAME, JOB_KEY 이 보입니다.\n배치 실행 시에는 JOB_NAME, JOB_KEY 를 기준으로 하나의 row 가 생성되는데, 이 JOB_NAME, JOB_KEY 는 중복을 허용하지 않습니다.\n즉, JobInstance 의 생성기준, BATCH_JOB_INSTANCE 테이블의 로우(ROW) 생성기준은 JOB_NAME, JOB_KEY이며 JOB_NAME, JOB_KEY은 중복되면 안됩니다.\nJOB_KEY 값은 BATCH_JOB_EXECUTION_PARAMS 테이블 내에 저장되는 Parameter 를 나열해서 암호화해서 저장합니다.\nBATCH_JOB_EXECUTION_PARAMS\nJob 을 실행할 때 사용된(주입된) Parameter를 저장하는 테이블입니다.\nBATCH_JOB_EXECUTION\nJob 의 시작시각, Job의 종료시각, Job 의 상태 를 기록하기 위한 테이블입니다.\nJob 이 실행되는 시점에 BATCH_JOB_EXECUTION 테이블에 데이터가 추가됩니다.\nBATCH_JOB_EXECUTION_CONTEXT\nJob 이 실행되는 동안 공유되어야 하는 데이터를 직렬화 해서 저장합니다.\nBATCH_STEP_EXECUTION\nStep 이 실행되는 동안 필요한 데이터와 실행 결과를 저장합니다.\nBATCH_STEP_EXECUTION_CONTEXT\nStep 이 실행되는 동안 공유되어야 하는 데이터를 직렬화해서 저장합니다.\n이 테이블에서는 하나의 Step 이 실행되는 동안 데이터를 공유합니다.\n이 테이블에서는 Step 간에 테이블의 데이터를 공유하지 않으며, Step 간에 데이터를 공유하는 것은 BATCH_JOB_EXECUTION_CONTEXT 에서 데이터를 공유할 수 있습니다.","schema-스크립트-경로#Schema 스크립트 경로":"테이블 ddl 이 있는 곳의 경로는 spring-batch-core/org.springframework/batch/core* 입니다.intellij 에서는 아래와 같이 찾으실 수 있습니다.Project View > External Libraries 에서 spring-batch-core 를 검색합니다.\n검색결과로 spring-batch-core-a.b.c.jar 파일이 검색되었음을 확인 가능합니다.\n스크롤을 내려보면 아래 그림처럼 schema-{db 타입}.sql 파일들이 나타나는 것을 볼 수 있습니다.위와 같이 Spring Batch 팀에서는 Spring Batch 라이브러리에 필요한 스키마들을 class path 내에서 찾을 수 있도록 제공해주고 있습니다.","batch-table-초기화-옵션#batch table 초기화 옵션":"batch 테이블 초기화 옵션은 application.yml 파일에 아래와 같이 설정할 수 있습니다.\nspring:\r\n  batch:\r\n    initialize-schema: never\n이 속성에는 아래와 같은 값들을 지정 가능합니다.\nspring.batch.initialize-schema=never\n배치 잡 기동시 스키마 초기화 스크립트를 실행하지 않는 방식입니다.\n주로 Production 레벨에서 사용하는 옵션입니다.\nspring.batch.initialize-schema=always\n배치 잡 기동시 스키마 초기화 스크립트를 항상 수행하도록 지정하는 방식입니다.\n주로 개발환경에서 사용하는 옵션입니다.\nspring.batch.initialize-schema=embedded\n배치 잡 기동시 스키마 초기화 스크립트를 h2 와 같은 embedded (내장) Database 를 사용할 때에만 사용할 수 있도록 지정하는 방식입니다.\n주로 개발환경에서 사용하는 옵션입니다.","객체-매핑#객체 매핑":"참고 : https://terasoluna-batch.github.io/guideline/5.0.0.RELEASE/en/Ch02_SpringBatchArchitecture.html#Ch02_SpringBatchArch_Detail_ProcessFlow\nJobInstance 객체 : BATCH_JOB_INSTANCE 테이블에 매핑\nJobExecution 객체 : BATCH_JOB_EXECUTION 테이블에 매핑\nJobParameters 객체 : BATCH_JOB_EXECUTION_PARAMS 테이블에 매핑\nExecutionContext 객체 : BATCH_JOB_EXECUTION_CONTEXT 테이블에 매핑","jobinstance-jobparameters-jobexecution#JobInstance, JobParameters, JobExecution":"JobInstanceJobInstance 는 BATCH_JOB_INSTANCE 테이블과 매핑되는 테이블입니다. JOB_INSTANCE 테이블의 주요 컬럼으로는 JOB_NAME, JOB_KEY 가 있습니다. 그리고 JOB_NAME, JOB_KEY 를 기준으로 하나의 row 가 생성되는데, 이 JOB_NAME, JOB_KEY 는 중복을 허용하지 않습니다. 즉, JobInstance 의 생성기준, BATCH_JOB_INSTANCE 테이블의 로우(ROW) 생성기준은 JOB_NAME, JOB_KEY이며 JOB_NAME, JOB_KEY은 중복되면 안됩니다.\nJobParameter, JobExecutionJobInstance 를 새로 생성할지에 대한 기준은 JobParameter 의 중복 여부로 결정합니다.\n같은 Parameter 로 Job을 다시 실행하면 이미 생성된 JobInstance 를 실행합니다.\n다른 Parameter 로 Job을 다시 실행하면 새로운 JobInstance 를 실행합니다.\nJobExecution 은 JobInstance 재실행 여부와 상관 없이 항상 새롭게 생성됩니다.e.g.\nJob 실행시 12월 1일 이라는 date parameter 를 처음 받아서 실행했다면?\njob_instance_id = k 에 해당하는 신규 JobInstance 실행\nJob 실행시 12월 2일 이라는 date parameter 를 처음 받아서 실행했다면?\njob_instance_id = m 에 해당하는 신규 JobInstance 실행\nJob 실행시 12월 2일 이라는 date parameter 를  한번 더 받아서 실행했다면?\njob_instance_id = m 에 해당하는 JobInstance 를 재실행","runidincmenter#RunIdIncmenter":"Job 을 항상 새로운 JobInstance로 실행되게끔 해야 할 경우가 있습니다. 이런 경우 RunIdIncrementer 를 사용합니다. RunIdIncrementer 를 사용하면 run.id 라는  job_key 에 대해 항상 다른 job_name 값이 지정되어서 항상 새로운 JobInstance 로 실행됩니다.","stepexecution-executioncontext#StepExecution, ExecutionContext":"StepExecution : BATCH_STEP_EXECUTION 테이블에 매핑되는 객체ExecutionContext : BATCH_STEP_EXECUTION_CONTEXT 테이블에 매핑되는 객체\nJOB, STEP 에 모두 매핑될 수 있는 객체입니다.\n참고) BATCH_JOB_EXECUTION_CONTEXT 테이블은 ExecutionContext 객체와 매핑됩니다.","요약#요약":"하나의 Job 은 항상 같은 파라미터로 새롭게 실행하는 것은 불가능합니다.ExecutionContext 는 Job, Step 의 Context 를 관리하는 객체입니다.\nExecutionContext 를 통해서 데이터를 공유할 수 있습니다.\nJobExecutionContext\nJob 내에서만 데이터를 공유할 수 있습니다\nJobExecutionContext 는 Step 끼리 데이터를 공유할 수 있습니다.\nStepExecutionContext\nStep 내에서만 데이터를 공유할 수 있습니다.\nStep 내에서만 공유하다는 의미이며, 다른 Step 과는 공유가 불가능합니다.","executioncontext#ExecutionContext":"Job 내에서 서로 다른 Step A, B 가 실행되게끔 Job 을 구성했다고 하겠습니다.\nTODO : 예제 추가 필요\nJobExecutionContext\nJob 내에서만 데이터를 공유할 수 있습니다\nJobExecutionContext 는 Step 끼리 데이터를 공유할 수 있습니다.\nStepExecutionContext\nStep 내에서만 데이터를 공유할 수 있습니다.\nStep 내에서만 공유하다는 의미이며, 다른 Step 과는 공유가 불가능합니다."}},"/todo/need-to-paging-based-processing":{"title":"Need to Paging Based Processing","data":{"가급적이면-cursoritemreader-기반-처리-대신-pagingitemreader-처리#가급적이면 CursorItemReader 기반 처리 대신 PagingItemReader 처리":"참고 : https://jiwondev.tistory.com/295JpaCursorItemReader 를 아무 생각 없이 사용하다가 사용자가 많이 없는데도 Database CPU 사용률이 높다는 알람을 받는 등과 같은 이슈가 발생할 수 있다배치 처리의 경우 jpa 를 사용할 경우 Bulk 단위 처리가 내부적으로는 잘 적용이 안되며, 어느 정도는 해킹수준의 적용이 필요하기에 가급적이면 JdbcTemplate 을 잘 활용하는 것이 추천됨"}},"/todo":{"title":"Todo","data":{"":"정리할 예정인 문서들"}}}